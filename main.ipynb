{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from pydub import AudioSegment\n",
    "from torch.utils.data import DataLoader\n",
    "from pydub.silence import detect_nonsilent\n",
    "\n",
    "# if not os.path.isfile(\"SUR_projekt2022-2023.zip\"):\n",
    "#     !wget https://www.fit.vutbr.cz/study/courses/SUR/public/projekt_2022-2023/SUR_projekt2022-2023.zip\n",
    "#     !unzip SUR_projekt2022-2023.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image import CustomDataset, augment_images, SmallCNNMultiClass, fit, eval, predict_image\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" \n",
    "\n",
    "CLASSES = 31\n",
    "data_augmentation_enabled = False\n",
    "\n",
    "# note that function will fail if augmentations were already present\n",
    "if data_augmentation_enabled:\n",
    "    augment_images('train', 'train/da')\n",
    "    augment_images('dev', 'dev/da', num_augmentations=int(1e2))\n",
    "\n",
    "def png_load(dir_name):\n",
    "    \"\"\"\n",
    "    Loads all *.png images\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    for f in glob(dir_name + '/*.png'):\n",
    "        features[f] = np.array(Image.open(f), dtype=np.float64)\n",
    "    return features\n",
    "\n",
    "train_x = np.empty((0,80,80,3))\n",
    "train_y = np.empty((0),dtype=int)\n",
    "\n",
    "test_x = np.empty((0,80,80,3))\n",
    "test_y = np.empty((0),dtype=int)\n",
    "\n",
    "for i in range(1,CLASSES+1):\n",
    "    train_i = np.array(list(png_load(os.path.join(\"train\",str(i))).values()))\n",
    "    label_i = np.full(len(train_i),i-1)\n",
    "    train_x = np.concatenate((train_x, train_i), axis=0)\n",
    "    train_y = np.concatenate((train_y, label_i), axis=0)\n",
    "\n",
    "    train_i = np.array(list(png_load(os.path.join(\"dev\",str(i))).values()))\n",
    "    label_i = np.full(len(train_i),i-1)\n",
    "    train_x = np.concatenate((train_x, train_i), axis=0)\n",
    "    train_y = np.concatenate((train_y, label_i), axis=0)\n",
    "\n",
    "    test_i = np.array(list(png_load(os.path.join(\"dev\",str(i))).values()))\n",
    "    label_i = np.full(len(test_i),i-1)\n",
    "    test_x = np.concatenate((test_x, test_i), axis=0)\n",
    "    test_y = np.concatenate((test_y, label_i), axis=0)\n",
    "\n",
    "print(\"Images were successfully loaded\")\n",
    "\n",
    "# convert 80,80,3 to 3,80,80\n",
    "train_x = np.array(train_x)\n",
    "train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
    "\n",
    "test_x = np.array(test_x)\n",
    "test_x = np.transpose(test_x, (0, 3, 1, 2))\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "train_tensors = torch.Tensor(train_x)\n",
    "test_tensors = torch.Tensor(test_x)\n",
    "\n",
    "\n",
    "# Create new TensorDataset instances with the modified labels\n",
    "train_dataset = CustomDataset(train_tensors, train_y)\n",
    "dev_dataset = CustomDataset(test_tensors, test_y)\n",
    "print(\"Dataset was successfully created\")\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model_name = 'model-0.9516.pt'\n",
    "\n",
    "if not os.path.isfile(model_name):\n",
    "    model = SmallCNNMultiClass().to(dev)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    criterion = F.cross_entropy\n",
    "    fit(200, model, optimizer, criterion, train_loader, dev_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), 'model.pt')\n",
    "else:\n",
    "    model = SmallCNNMultiClass()\n",
    "    model.load_state_dict(torch.load(model_name, map_location=torch.device(dev)))\n",
    "    model.to(dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "test_x = np.empty((0,80,80,3))\n",
    "test_x = np.array(list(png_load('eval').values()))\n",
    "test_x = np.transpose(test_x, (0, 3, 1, 2))\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(test_x))\n",
    "test_loader = DataLoader(test_dataset, batch_size=736)\n",
    "\n",
    "for x in test_loader:\n",
    "    pred = model(x[0])\n",
    "    _, pred = torch.max(pred, dim=1)\n",
    "pred = pred + 1\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ikrlib as ilib\n",
    "\n",
    "from audio import audio_adjust, reduce_noise, data_augumentation, pre_emphasis, min_max_normalize, compute_deltas, cepstral_mean_subtraction\n",
    "\n",
    "cepstral_mean_subtraction_enabled = False\n",
    "delta_coefficients_enabled = False\n",
    "coefficients_normalization = False\n",
    "\n",
    "audio_adjust_enabled = True\n",
    "reduce_noise_enabled = True\n",
    "data_augmentation_enabled = True\n",
    "data_pre_emphasis = False\n",
    "\n",
    "if audio_adjust_enabled:\n",
    "    for i in range(1, 32):\n",
    "        audio_adjust(ilib.get_directory(f\"train/{i}\"))\n",
    "        audio_adjust(ilib.get_directory(f\"dev/{i}\"))\n",
    "    \n",
    "    audio_adjust(ilib.get_directory(f\"eval\"))\n",
    "print(\"Silence was successfully removed\")\n",
    "\n",
    "\n",
    "if reduce_noise_enabled:\n",
    "    for i in range(1, 32):\n",
    "        reduce_noise(ilib.get_directory(f\"train/{i}\", audio_adjust_enabled))\n",
    "        reduce_noise(ilib.get_directory(f\"dev/{i}\", audio_adjust_enabled))\n",
    "    reduce_noise(ilib.get_directory(f\"eval\", audio_adjust_enabled))\n",
    "    print(\"Noise was successfully removed\")\n",
    "\n",
    "if data_augmentation_enabled:\n",
    "    for i in range(1, 32):\n",
    "        data_augumentation(ilib.get_directory(f\"train/{i}\", audio_adjust_enabled, reduce_noise_enabled))\n",
    "    print(\"Data augumentation was done\")\n",
    "\n",
    "if data_pre_emphasis:\n",
    "    train = {}\n",
    "    dev = {}\n",
    "    for i in range(1, 32):\n",
    "        train[i] =  np.vstack(pre_emphasis(ilib.get_directory(f'train/{i}', audio_adjust_enabled, reduce_noise_enabled, data_augmentation_enabled)))\n",
    "        dev[i] =  list(pre_emphasis(ilib.get_directory(f'dev/{i}', audio_adjust_enabled, reduce_noise_enabled)))\n",
    "    print(\"Pre emphasis was successfull\")\n",
    "\n",
    "if not data_pre_emphasis:\n",
    "    train = {}\n",
    "    dev = {}\n",
    "    for i in range(1, 32):\n",
    "        train[i] = np.vstack(list(ilib.wav16khz2mfcc(ilib.get_directory(f'train/{i}', audio_adjust_enabled, reduce_noise_enabled, data_augmentation_enabled)).values()))\n",
    "        dev[i] = list(ilib.wav16khz2mfcc(ilib.get_directory(f'train/{i}', audio_adjust_enabled, reduce_noise_enabled)).values())\n",
    "    print(\"Loading data was successful\")\n",
    "\n",
    "if coefficients_normalization:\n",
    "    for i in range(1, 32):\n",
    "        train[i] = min_max_normalize(train[i])\n",
    "\n",
    "if delta_coefficients_enabled:\n",
    "    for i in range(1, 32):\n",
    "        train_delta_coeffs = compute_deltas(train[i], window_size=2)\n",
    "        train_derivative_delta_coeffs = compute_deltas(train[i], window_size=2)\n",
    "        train[i] = np.concatenate((train[i], train_delta_coeffs, train_derivative_delta_coeffs), axis=1)\n",
    "\n",
    "if cepstral_mean_subtraction_enabled:\n",
    "    for i in range(1, 32):\n",
    "        train[i] = cepstral_mean_subtraction(train[i])\n",
    "\n",
    "M = 3  # Počet gaussovských komponent\n",
    "MUs = {}\n",
    "COVs = {}\n",
    "Ws = {}\n",
    "for i in range(1, 32):\n",
    "    MUs[i] = train[i][np.random.randint(1, len(train[i]), M)]  # Počiatočna stredná hodnota\n",
    "    #COVs[i] = [np.cov(train[i].T)] * M  # Počiatočna kovariančná matica\n",
    "    COVs[i] = [np.diag(np.diag(np.cov(train[i].T))) for _ in range(M)]  # Initial diagonal covariance matrix\n",
    "    Ws[i] = np.ones(M) / M\n",
    "\n",
    "for jj in range(30):\n",
    "    # TTL_t je doveryhodnosť\n",
    "    for i in range(1, 32):\n",
    "        Ws[i], MUs[i], COVs[i], TTL = ilib.train_gmm(train[i], Ws[i], MUs[i], COVs[i])\n",
    "        print(f'Iteration: {jj} Total log likelihood: {TTL} for person {i}')\n",
    "\n",
    "P_t=0.5\n",
    "P_n=1.0-P_t\n",
    "\n",
    "score = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for true_class in range(1, 32):\n",
    "    for dev_p_i in dev[true_class]:\n",
    "        dev_p_i_cpy = dev_p_i.copy()\n",
    "\n",
    "        if coefficients_normalization:\n",
    "            dev_p_i_cpy = min_max_normalize(dev_p_i_cpy)\n",
    "\n",
    "        if delta_coefficients_enabled:\n",
    "            test_t_delta_coeffs = compute_deltas(dev_p_i_cpy, window_size=2)\n",
    "            test_t_derivative_delta_coeffs = compute_deltas(test_t_delta_coeffs, window_size=2)\n",
    "\n",
    "            dev_p_i_cpy = np.concatenate((dev_p_i_cpy, test_t_delta_coeffs, test_t_derivative_delta_coeffs), axis=1)\n",
    "\n",
    "        if cepstral_mean_subtraction_enabled:\n",
    "            dev_p_i_cpy = cepstral_mean_subtraction(dev_p_i_cpy)\n",
    "\n",
    "        # Compute the likelihoods for all the classes\n",
    "        likelihoods = np.array([ilib.logpdf_gmm(dev_p_i_cpy, Ws[i], MUs[i], COVs[i]).sum() for i in range(1, 32)])\n",
    "\n",
    "        # Find the class with the highest likelihood\n",
    "        predicted_class = np.argmax(likelihoods) + 1\n",
    "\n",
    "        # Compare the predicted class with the true class\n",
    "        if predicted_class == true_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Fraction of correctly recognized targets: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
