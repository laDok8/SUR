{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import ikrlib as ikrl\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from augment import augment_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation_enabled = True\n",
    "\n",
    "if data_augmentation_enabled:\n",
    "    augment_images('train', 'train/da')\n",
    "    augment_images('dev', 'dev/da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images were successfully loaded\n",
      "Dataset was successfully created\n",
      "Epoch: 1/500, Loss: 55.0295, Accuracy: 0.0161, 1 and 62\n",
      "Epoch: 51/500, Loss: 44.8971, Accuracy: 0.7258, 45 and 62\n",
      "Epoch: 101/500, Loss: 43.9168, Accuracy: 0.8226, 51 and 62\n",
      "Epoch: 151/500, Loss: 43.1473, Accuracy: 0.8548, 53 and 62\n",
      "Epoch: 201/500, Loss: 44.3219, Accuracy: 0.8548, 53 and 62\n",
      "Epoch: 251/500, Loss: 43.2679, Accuracy: 0.9194, 57 and 62\n",
      "Epoch: 301/500, Loss: 42.5740, Accuracy: 0.9355, 58 and 62\n",
      "Epoch: 351/500, Loss: 43.1705, Accuracy: 0.9194, 57 and 62\n",
      "Epoch: 401/500, Loss: 43.5827, Accuracy: 1.0000, 62 and 62\n",
      "Epoch: 451/500, Loss: 43.3906, Accuracy: 1.0000, 62 and 62\n",
      "Epoch: 500/500, Loss: 43.3262, Accuracy: 1.0000, 62 and 62\n"
     ]
    }
   ],
   "source": [
    "from image import CustomDataset, SmallCNNMultiClass, train\n",
    "\n",
    "CLASSES = 31\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\" \n",
    "\n",
    "train_x = np.empty((0,80,80,3))\n",
    "train_y = np.empty((0),dtype=int)\n",
    "\n",
    "test_x = np.empty((0,80,80,3))\n",
    "test_y = np.empty((0),dtype=int)\n",
    "\n",
    "for i in range(1,CLASSES+1):\n",
    "    train_i = np.array(list(ikrl.png_load(os.path.join(\"train\",str(i)), False).values()))\n",
    "    label_i = np.full(len(train_i),i-1)\n",
    "    train_x = np.concatenate((train_x, train_i), axis=0)\n",
    "    train_y = np.concatenate((train_y, label_i), axis=0)\n",
    "\n",
    "    test_i = np.array(list(ikrl.png_load(os.path.join(\"dev\",str(i)), False).values()))\n",
    "    label_i = np.full(len(test_i),i-1)\n",
    "    test_x = np.concatenate((test_x, test_i), axis=0)\n",
    "    test_y = np.concatenate((test_y, label_i), axis=0)\n",
    "\n",
    "print(\"Images were successfully loaded\")\n",
    "\n",
    "# convert 80,80,3 to 3,80,80\n",
    "train_x = np.array(train_x)\n",
    "train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
    "\n",
    "test_x = np.array(test_x)\n",
    "test_x = np.transpose(test_x, (0, 3, 1, 2))\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "train_tensors = torch.Tensor(train_x)\n",
    "test_tensors = torch.Tensor(test_x)\n",
    "\n",
    "\n",
    "# Create new TensorDataset instances with the modified labels\n",
    "train_dataset = CustomDataset(train_tensors, train_y)\n",
    "test_dataset = CustomDataset(test_tensors, test_y)\n",
    "print(\"Dataset was successfully created\")\n",
    "\n",
    "model = SmallCNNMultiClass()\n",
    "criterion = F.cross_entropy\n",
    "model = model.to(dev)\n",
    "# model = Net().to(dev)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "accuracys, losses = train(model, train_dataset, test_dataset, optimizer, criterion, dev, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "test_x = np.empty((0,80,80,3))\n",
    "test_x = np.array(list(ikrl.png_load('eval', False).values()))\n",
    "test_x = np.transpose(test_x, (0, 3, 1, 2))\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(test_x))\n",
    "test_loader = DataLoader(test_dataset, batch_size=736)\n",
    "\n",
    "for x in test_loader:\n",
    "    pred = model(x[0].to(dev))\n",
    "    _, pred = torch.max(pred, dim=1)\n",
    "pred = pred + 1\n",
    "print(pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images were successfully loaded\n",
      "[[0.06018147 0.09588032 0.019359   0.03357952 0.01881544 0.0235429\n",
      "  0.16910213 0.01118661 0.04132677 0.01328554 0.01170306 0.02117478\n",
      "  0.13587252 0.01542163 0.00599437 0.03110354 0.04310296 0.01890837\n",
      "  0.00945689 0.03709076 0.00924805 0.00837855 0.00654431 0.00542381\n",
      "  0.01984617 0.03578422 0.04744238 0.02587904 0.00577893 0.01391617\n",
      "  0.00566977]]\n"
     ]
    }
   ],
   "source": [
    "from svm import SVCTrain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from image import CustomDataset\n",
    "CLASSES = 31\n",
    "\n",
    "def png_load(dir_name):\n",
    "    \"\"\"\n",
    "    Loads all *.png images from directory dir_name into a dictionary. Keys are the file names\n",
    "    and values and 2D numpy arrays with corresponding grayscale images\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    for f in glob(dir_name + '/*.png'):\n",
    "        features[f] = np.array(Image.open(f), dtype=np.float64)\n",
    "    return features\n",
    "\n",
    "train_x = np.empty((0,80,80,3))\n",
    "train_y = np.empty((0),dtype=int)\n",
    "\n",
    "for i in range(1,CLASSES+1):\n",
    "    train_i = np.array(list(png_load(os.path.join(\"train\",str(i))).values()))\n",
    "    label_i = np.full(len(train_i),i-1)\n",
    "    train_x = np.concatenate((train_x, train_i), axis=0)\n",
    "    train_y = np.concatenate((train_y, label_i), axis=0)\n",
    "\n",
    "train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
    "\n",
    "print(\"Images were successfully loaded\")\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3,random_state=109)\n",
    "\n",
    "train_dataset = CustomDataset(train_x, train_y)\n",
    "\n",
    "model = SVCTrain()\n",
    "model.train(train_dataset, CustomDataset(test_x, test_y))\n",
    "print(model.predict(test_x[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data was successful\n",
      "Creating subs mean classes\n",
      "Training GMM\n",
      "Evaluating GMM classes\n",
      "Accuracy: 0.12903225806451613\n"
     ]
    }
   ],
   "source": [
    "from gmm_image import train_gmm, eval, augment_images_gmm\n",
    "\n",
    "CLASSES = 31\n",
    "\n",
    "data_augmentation_enabled = False\n",
    "\n",
    "if data_augmentation_enabled:\n",
    "    for i in range(1,CLASSES+1):\n",
    "        augment_images_gmm(f\"train/{i}\", f\"train/{i}/da\", 3)\n",
    "\n",
    "dev_subs_mean, ws_list, mus_list, covs_list = train_gmm()\n",
    "eval(dev_subs_mean, ws_list, mus_list, covs_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from audio import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cepstral_mean_subtraction_enabled = False\n",
    "delta_coefficients_enabled = False\n",
    "coefficients_normalization = False\n",
    "\n",
    "audio_adjust_enabled = True\n",
    "reduce_noise_enabled = True\n",
    "data_augmentation_enabled = True\n",
    "data_pre_emphasis = False\n",
    "\n",
    "CLASSES = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing silence from records in directory train/1/\n",
      "Removing silence from records in directory eval/1/\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eval/1/rs/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m audio \u001b[39m=\u001b[39m Audio(CLASSES, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m audio\u001b[39m.\u001b[39;49mdo_audio_adjust(audio_adjust_enabled)\n\u001b[1;32m      3\u001b[0m audio\u001b[39m.\u001b[39mdo_reduce_noise(reduce_noise_enabled)\n\u001b[1;32m      4\u001b[0m audio\u001b[39m.\u001b[39mdo_data_augmentation(data_augmentation_enabled)\n",
      "File \u001b[0;32m~/SUR/audio.py:151\u001b[0m, in \u001b[0;36mAudio.do_audio_adjust\u001b[0;34m(self, audio_adjust_enabled)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCLASSES \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    150\u001b[0m         Audio\u001b[39m.\u001b[39maudio_adjust(ilib\u001b[39m.\u001b[39mget_directory(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 151\u001b[0m         Audio\u001b[39m.\u001b[39;49maudio_adjust(ilib\u001b[39m.\u001b[39;49mget_directory(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdev\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    152\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSilence was successfully removed\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/SUR/audio.py:32\u001b[0m, in \u001b[0;36mAudio.audio_adjust\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRemoving silence from records in directory \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mdir\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(new_dir):\n\u001b[0;32m---> 32\u001b[0m     os\u001b[39m.\u001b[39;49mmkdir(new_dir)\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(\u001b[39mdir\u001b[39m):\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m f[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwav\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eval/1/rs/'"
     ]
    }
   ],
   "source": [
    "audio = Audio(CLASSES, 'train', 'eval')\n",
    "audio.do_audio_adjust(audio_adjust_enabled)\n",
    "audio.do_reduce_noise(reduce_noise_enabled)\n",
    "audio.do_data_augmentation(data_augmentation_enabled)\n",
    "if data_pre_emphasis:\n",
    "    train_audio, dev_audio = audio.do_data_pre_emphasis()\n",
    "else:\n",
    "    train_audio, dev_audio = audio.do_classic_load()\n",
    "train_audio = audio.do_coefficients_normalization(train_audio, coefficients_normalization)\n",
    "train_audio = audio.do_delta_coefficients(train_audio, delta_coefficients_enabled)\n",
    "train_audio = audio.do_cepstral_mean_subtraction(train_audio, cepstral_mean_subtraction_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws, MUs, COVs = audio.train(train_audio, 3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes, accuracy = audio.eval(dev_audio, Ws, MUs, COVs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
